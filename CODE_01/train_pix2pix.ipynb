{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096c8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "from ignite.engine import Engine\n",
    "from ignite.engine import Events\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.handlers import Timer\n",
    "from ignite.contrib.handlers.tensorboard_logger import OutputHandler\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger\n",
    "import structlog\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from pix2pix import Discriminator\n",
    "from pix2pix import Generator\n",
    "from data import XView2Dataset\n",
    "\n",
    "LOGGER = structlog.get_logger()\n",
    "\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "TEST_BATCH_SIZE = 1\n",
    "GENERATOR_FILTERS = 64\n",
    "DISCRIMINATOR_FILTERS = 64\n",
    "TRAIN_EPOCHS = 200\n",
    "GENERATOR_LR = 0.0002\n",
    "DISCRIMINATOR_LR = 0.0002\n",
    "L1_LAMBDA = 100\n",
    "BETA_1 = 0.5\n",
    "BETA_2 = 0.999\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"data_directory\", type=Path)\n",
    "    parser.add_argument(\"--generator-weights\", type=Path)\n",
    "    parser.add_argument(\"--discriminator-weights\", type=Path)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    generator = Generator(GENERATOR_FILTERS)\n",
    "    if args.generator_weights is not None:\n",
    "        LOGGER.info(f\"Loading generator weights: {args.generator_weights}\")\n",
    "        generator.load_state_dict(torch.load(args.generator_weights))\n",
    "    else:\n",
    "        generator.weight_init(mean=0.0, std=0.02)\n",
    "\n",
    "    discriminator = Discriminator(DISCRIMINATOR_FILTERS)\n",
    "    if args.discriminator_weights is not None:\n",
    "        LOGGER.info(f\"Loading discriminator weights: {args.discriminator_weights}\")\n",
    "        discriminator.load_state_dict(torch.load(args.discriminator_weights))\n",
    "    else:\n",
    "        discriminator.weight_init(mean=0.0, std=0.02)\n",
    "\n",
    "    dataset = XView2Dataset(\n",
    "        args.data_directory,\n",
    "    )\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 10, 10])\n",
    "\n",
    "    # Create a dev train dataset with just 10 samples\n",
    "    # train_dataset, _ = torch.utils.data.random_split(train_dataset, [10, len(train_dataset) - 10])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=TRAIN_BATCH_SIZE\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=TEST_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    BCE_loss = nn.BCELoss().cuda()\n",
    "    L1_loss = nn.L1Loss().cuda()\n",
    "\n",
    "    generator_optimizer = optim.Adam(generator.parameters(), lr=GENERATOR_LR, betas=(BETA_1, BETA_2))\n",
    "    discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=DISCRIMINATOR_LR, betas=(BETA_1, BETA_2))\n",
    "\n",
    "    def step(engine, batch):\n",
    "        x, y = batch\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        discriminator.zero_grad()\n",
    "        discriminator_result = discriminator(x, y).squeeze()\n",
    "        discriminator_real_loss = BCE_loss(discriminator_result, torch.ones(discriminator_result.size()).cuda())\n",
    "\n",
    "        generator_result = generator(x)\n",
    "        discriminator_result = discriminator(x, generator_result).squeeze()\n",
    "\n",
    "        discriminator_fake_loss = BCE_loss(discriminator_result, torch.zeros(discriminator_result.size()).cuda())\n",
    "        discriminator_train_loss = (discriminator_real_loss + discriminator_fake_loss) * 0.5\n",
    "        discriminator_train_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        generator.zero_grad()\n",
    "        generator_result = generator(x)\n",
    "        # TODO Work out if the below time saving technique impacts training.\n",
    "        #generator_result = generator_result.detach()\n",
    "        discriminator_result = discriminator(x, generator_result).squeeze()\n",
    "\n",
    "        l1_loss = L1_loss(generator_result, y)\n",
    "        bce_loss = BCE_loss(discriminator_result, torch.ones(discriminator_result.size()).cuda())\n",
    "\n",
    "        G_train_loss = bce_loss + L1_LAMBDA * l1_loss\n",
    "        G_train_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        return {\n",
    "            'generator_train_loss': G_train_loss.item(),\n",
    "            'discriminator_real_loss': discriminator_real_loss.item(),\n",
    "            'discriminator_fake_loss': discriminator_fake_loss.item(),\n",
    "        }\n",
    "\n",
    "    trainer = Engine(step)\n",
    "\n",
    "    tb_logger = TensorboardLogger(log_dir=f\"tensorboard/logdir/{uuid4()}\")\n",
    "    tb_logger.attach(\n",
    "        trainer,\n",
    "        log_handler=OutputHandler(\n",
    "            tag=\"training\",\n",
    "            output_transform=lambda out: out,\n",
    "            metric_names='all'\n",
    "        ),\n",
    "        event_name=Events.ITERATION_COMPLETED\n",
    "    )\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def add_generated_images(engine):\n",
    "        def min_max(image):\n",
    "            return (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "        for idx, (x, y) in enumerate(test_loader):\n",
    "            generated = min_max(generator(x.cuda()).squeeze().cpu())\n",
    "            real = min_max(y.squeeze())\n",
    "\n",
    "            tb_logger.writer.add_image(\n",
    "                f\"generated_test_image_{idx}\",\n",
    "                # Concatenate the images into a single tiled image\n",
    "                torch.cat([x.squeeze(), generated, real], 2),\n",
    "                global_step=engine.state.epoch\n",
    "            )\n",
    "\n",
    "    checkpoint_handler = ModelCheckpoint(\n",
    "        \"checkpoints/\", \"pix2pix\",\n",
    "        n_saved=1, require_empty=False, save_interval=1\n",
    "    )\n",
    "    trainer.add_event_handler(\n",
    "        event_name=Events.EPOCH_COMPLETED,\n",
    "        handler=checkpoint_handler,\n",
    "        to_save={\n",
    "            'generator': generator,\n",
    "            'discriminator': discriminator\n",
    "        })\n",
    "\n",
    "    timer = Timer(average=True)\n",
    "    timer.attach(\n",
    "        trainer,\n",
    "        resume=Events.ITERATION_STARTED,\n",
    "        step=Events.ITERATION_COMPLETED\n",
    "    )\n",
    "\n",
    "    @trainer.on(Events.ITERATION_COMPLETED)\n",
    "    def log_training_loss(engine):\n",
    "        print(\n",
    "            \"Epoch[{}] Iteration[{}] Duration[{}] Losses: {}\".format(\n",
    "                engine.state.epoch,\n",
    "                engine.state.iteration,\n",
    "                timer.value(),\n",
    "                engine.state.output\n",
    "            )\n",
    "        )\n",
    "\n",
    "    trainer.run(train_loader, max_epochs=TRAIN_EPOCHS)\n",
    "\n",
    "    tb_logger.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
